\section{Experiment}
\label{sec:exp}
We conducted extensive experiments
to evaluate the performance of HINGCN.
This section summarizes our results. 
We compare the various methods using three popular measures, 
namely, \emph{accuracy}, \emph{micro-F1}, and \emph{macro-F1}.
These measures evaluate clustering quality and their values range from 0 to 1, 
with a larger value indicating a better
clustering quality. 
%We first describe the performance measures (Section~\ref{sec:measures}) and
%existing methods against which ROSC is compared (Section~\ref{sec:algo-comp}).
%We then show the performance results on both real and synthetic datasets (Section~\ref{sec:results}).
%We illustrate the grouping effect of matrix $\tilde{Z}$ (Section~\ref{}).
%Finally, we conduct a parameter sensitivity study of ROSC (Section~\ref{}).

\subsection{Datasets}
We use three datasets, namely DBLP \footnote{https://dblp.uni-trier.de/}, Yelp\footnote{https://www.yelp.com/academic{\_}dataset/} and Freebase \footnote{https://www.freebase.com/}. A summary of statistics of the datasets are shown in table \ref{table:dataset}.

\noindent{\small$\bullet$}
\textbf{DBLP }: We extract a subset of DBLP which contains 4057 authors ($A$), 14328 papers ($P$) and 20 conferences ($C$). Authors are classified into four areas: \textit{database}, \textit{data mining}, \textit{machine learning}, \textit{information retrieval}. In addition, 8789 keyword terms are assigned to each author as feature, following tf-idf transformation. 
Links include A-P ( author publishes a paper) and P-C (paper published at a conference).
We consider meta-paths $\{APA,APAPA,APCPA\}$ for experiments. The task of semi-supervised classification is to predict which area of research an author is focusing on.
We obtained the ground truth from the dataset dblp-4area \cite{SunYH09}, which
labels each author by his/her primary research area.

\noindent{\small$\bullet$}
\textbf{Yelp-Restaurant}: We extracted information related to restaurant businesses in YELP. From extracted information, we constructed a dataset which contains 2614 business objects ($B$); 33360 review objects ($R$); 1286 user objects ($U$) and 82 food relevant keyword objects ($K$).
Restaurant businesses falls into three categories: "Fast Food", "Sushi Bars" and "American (New) Food".
Each business object is associated with 3 categorical attributes which includes reservation (whether reservation is required), service (waiter service or self service) and parking; as well as 1 numerical attribute: review count; and 1 ordinal attribute: quality star.
Links include B-R (business receives a review), U-R (user writes a review), K-R (keyword included in a review). We consider the meta-path set $\{BRURB, BRKRB\}$. The semi-supervised classification task is to classify business objects by state. We use the class information provided in the dataset as the ground truth.

\noindent{\small$\bullet$}
\textbf{Freebase-Movie}: We extract a subset of Freebase movie data which contains ? movies ($M$); ? actors ($A$); ? directors ($D$) and ? writers ($W$). The movies are divided into three classes: \textit{Action}, \textit{Comedy} and \textit{Drama}. 
%Each movie has a 5-dimensional meta-data as feature.
No attribute is provided with movies. 
Links include M-A (movie and its actor), M-D (movie and its director), M-W (movie and its writer).
We consider meta-paths $\{MAM,MDM,MWM\}$ for experiments. The task of semi-supervised classification is to predict the class of movies. We use the class information provided in the dataset as the ground truth.

\comment{
\subsection{Measures}
\label{sec:measures}
We use three popular measures, namely, \emph{purity}, \emph{adjusted mutual information (AMI)}, and \emph{rand index (RI)}, to evaluate clustering quality~\cite{vinh2010information,lin2010power}.

Consider a clustering $\mathcal{C} = \{C_1, \ldots, C_k\}$ produced by a clustering algorithm
and a gold standard (true) clustering
$\mathcal{C}_t = \{ \hat{C}_1, \ldots, \hat{C}_k\}$.
For each cluster $C_i \in \mathcal{C}$, we find the cluster $\hat{C}_j \in \mathcal{C}_t$ that overlaps
with $C_i$ the most. 
The purity of cluster $C_i$ is the fraction of objects in $C_i$ that fall in the overlap, i.e., 
($\max_j |C_i \cap \hat{C}_j|) / |C_i|$. 
The purity of a clustering $\mathcal{C}$ is the average of its clusters' purities, weighted by the cluster sizes:
\begin{equation}
purity(\mathcal{C}_t,\mathcal{C}) = \frac{1}{n}\sum_{i}\max_{j}|C_i \cap \hat C_j|.
\end{equation}
The adjusted mutual information ({\it AMI}) is mutual information with the agreement due to chance between clusterings corrected, and
is given by,
\begin{equation}
\mathit{AMI}(\mathcal{C}_t,\mathcal{C}) = \frac{MI(\mathcal{C}_t,\mathcal{C}) - E\{MI(\mathcal{C}_t,\mathcal{C})\}}{\max\{H(\mathcal{C}_t),H(\mathcal{C})\} - E\{MI(\mathcal{C}_t,\mathcal{C})\}},
\end{equation}
where $MI(\mathcal{C}_t,\mathcal{C})$ is the mutual information between $\mathcal{C}_t$ and $\mathcal{C}$,
$H(\mathcal{C}_t)$ and $H(\mathcal{C})$ are the entropies of $\mathcal{C}_t$ and $\mathcal{C}$, respectively,
and $E\{MI(\mathcal{C}_t,\mathcal{C})\}$ is the expected mutual information between the two clusterings
$\mathcal{C}_t$ and $\mathcal{C}$.

Rand index ({\it RI}) considers object pairs in measuring clustering quality. It is defined as:
\begin{equation}
RI(\mathcal{C}_t,\mathcal{C}) = (N_{00} + N_{11}) / {\tbinom n2},
\end{equation}
where $N_{00}$ is the number of object pairs that are put into the same cluster in $\mathcal{C}_t$
as well as in the same cluster in $\mathcal{C}$, and
$N_{11}$ is the number of object pairs that are put into different clusters in 
$\mathcal{C}_t$ and also in different clusters in $\mathcal{C}$.
Note that values of all three measures range from 0 to 1, with a larger value indicating a better
clustering quality. 
}

\subsection{Algorithms for comparison}
\label{sec:algo-comp}
We evaluate HINGCN and 9 other methods. To demonstrate effectiveness of our edge update mechanism and meta-path GLU layer, we also includes two variants of HINGCN.

\noindent{\small$\bullet$}
\textbf{Node2vec}: A standard spectral clustering method with symmetric normalization.

\noindent{\small$\bullet$}
\textbf{Metpath2vec}: A standard spectral clustering method with divisive normalization.

\noindent{\small$\bullet$}
\textbf{GCN}: A self-tuning spectral clustering method for multi-scale clusters.
It uses eigenvector rotation to estimate the number of clusters.
To make a fair comparison, we directly set $k$ as in other methods.

\noindent{\small$\bullet$}
\textbf{GAT}: A power iteration based method which generates only one pseudo-eigenvector.

\noindent{\small$\bullet$}
\textbf{HAN}:
A hierarchical semi-supervised graph attention network that employs node-level attention and meta-path level attention.

\noindent{\small$\bullet$}
\textbf{HINGCN$_ed$}: 
A variant of HINGCN which replace update of edge embedding by identity propagation.

\noindent{\small$\bullet$}
\textbf{HINGCN$_at$}:
A variant of HINGCN which uses attention mechanism for meta-path level aggregation.

\noindent{\small$\bullet$}
\textbf{HINGCN}:
The proposed semi-supervised classification method based on heterogeneous graph convolution networks.


\subsection{Experiment setup}
\label{sec:setup}
\dan{TODO}
We implement HINGCN using PyTorch. We use the same learning rate 0.001 for all HINGCN variants. The models  are trained with two NVIDIA 1080Ti GPUs using data parallelism. The batch size is 1024 for each GPU. We stack two graph aggregators, and each aggregator samples 16 neighbors for both training and testing. Each aggregator is then followed by a ReLU activation layer and a dropout layer with dropout rate 0.5. Output dimension of the layers are set to 64 and 32 respectively. For dataset without node features, we encode a 1-hot matrix as input feature. As mentioned in section \dan{TODO}, we ensemble edge feature from pre-trained path-sim and node embeddings. These node embeddings are trained using Node2vec with default parameters on the underlying graph neglecting heterogeneity.

The parameters of all the baseline methods are set according to their original papers.
For each method and dataset,
we run the experiment 10 times and report average results.

\subsection{Performance results}
\label{sec:results}

We apply all 10 methods on \textsc{DBLP}. 
Due to space limitations, for each category of methods, we only show the best performing one. 
They are, namely, NJW, PIC-$k$, FUSE, and ROSC. 

\subsection{Analysis of HINGCN layers}
We provide a detailed analysis of each component layer of HINGCN model.

\textbf{Analysis of edge feature.}

\textbf{Analysis of effect of updating edge.}

\textbf{Analysis of different meta-path aggregation units.}

\subsection{Sensitivity Experiments of Hyper-parameters}
In this section, we investigate the sensitivity of hyper-parameters. We vary concerned parameters and compare against default settings described in Sec.~\ref{sec:setup} 

\textbf{Dimension of edge feature.}

\textbf{Number of sampled neighbors.}






