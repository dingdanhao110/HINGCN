\section{Algorithm}
\label{sec:algorithm}

In this section we describe our HINGCN algorithm. Figure~\ref{figure:flow_graph}(c) shows a flow diagram of HINGCN. 
\dan{TODO:signposting}

\subsection{Edge features as supplementary information}
\label{sec:edge}
According to research findings of Xu et al. \cite{XuHLJ19}, discriminative power of a graph neural network determines its representation capability.
We claim that information is loss if we only consider adjacency of target type vertices. 
\dan{TODO:relate this example to the example figure used in preliminary section}
For example, in long tailed meta-path $APCPA$, a shrunk $A$-$A$ adjacency loses information from intermediate $PCP$ nodes. And a graph neural network using only adjacency information cannot distinguish between two instance $p_1 = (a_1 p_1 c_1 p_2 a_2)$ and $p_2 = (a_1 p_3 c_2 p_4 a_2)$. 
Therefore we propose to enhance expressivity of our network by adding additional edge feature to homogeneous $A$-$A$ edges. These edge features are designed to effectly distinguish different intermediate nodes between meta-path instances.
We propose edge features that is composed of three different components, namely path count, intermediate node embeddings and PathSim\citep{SunHYYW11}.

\noindent{\small$\bullet$}\textbf{Path Count}. Different object pairs have different number of path instance between them and this number may reflect object similarity under a given meta-path. We propose to record this count of path between objects $x_u$ and $x_v$ under meta-path $\Phi$ by: 
\begin{equation*}
c(x_u,x_v) = \vert\{ p_{x_u \rightsquigarrow x_v}:p_{x_u \rightsquigarrow x_v} \vdash \Phi \}\vert
\end{equation*}

\noindent{\small$\bullet$}\textbf{Intermediate object embedding}. 
To discriminate between different intermediate objects, we propose to use a sum of pre-trained node embedding as a component of edge feature. This node embedding can be trained from heterogeneous embedding algorithms such as \cite{GroverL16,DongCS17}.
Given a meta-path instance $p = (x_1x_2\ldots x_{l+1})$, we calculate
\begin{equation*}
o(x_1,x_{l+1}) = \dfrac{1}{l \cdot c(x_1,x_{l+1})} \sum\limits_{i=1}^{l+1} e_i
\end{equation*}
where $e_i$ is embedding of object $x_i$. Division of Path length $l$ and path count $c(x_u,x_v)$ is proposed to normalize intermediate object embedding to the scale of input object embeddings. Since path count is extracted and recorded as $c(x_u,x_v)$, there is no additional loss of in doing so.

\noindent{\small$\bullet$}\textbf{PathSim}. 
PathSim is proposed in \citep{SunHYYW11} to measure similarity between two objects of a same type on a heterogeneous graph. Given a symmetric meta-path $\Phi$, PathSim between $x_u$ and $x_v$ is:
\begin{equation*}
s(x_u,x_v) = \frac{2\times\vert\{ p_{x_u \rightsquigarrow x_v}:p_{x_u \rightsquigarrow x_v} \vdash \Phi \}\vert}{\vert\{ p_{x_u \rightsquigarrow x_u}:p_{x_u \rightsquigarrow x_u} \vdash \Phi \}\vert +\vert\{ p_{x_v \rightsquigarrow x_v}:p_{x_v \rightsquigarrow x_v} \vdash \Phi \}\vert }
\end{equation*}

The edge feature between objects $x_u$ and $x_v$ under meta-path $\Phi$ is therefore concatenated as:
\begin{equation}
\label{eq:edge}
r^{(0)}_\Phi(x_u,x_v) = c(x_u,x_v)\oplus o(x_u,x_v)\oplus s(x_u,x_v)
\end{equation}

\subsection{Aggregating node embedding}

\dan{TODO}
We utilize a dot product attention mechanism to aggregate node embedding from its neighbor. Given a meta-path $\Phi$, we update vertex embedding as follow:
\begin{equation}
\label{eq:dot}
\alpha^{(t)}_{i,j} = [W_{a1}^{(t)}(\text{tanh}(W_{a2}^{(t)}h^{(t)}_i ))]^T \cdot W_{a3}^{(t)}(\text{tanh}(W_{a4}^{(t)}(h^{(t)}_j \oplus r^{(t)}_{i,j}) ))
\end{equation}

\begin{equation}
\label{eq:softmax}
a^{(t)}_{i,j} = \text{softmax}(\alpha^{(t)}_{i,j}) = \dfrac{\text{exp}(\alpha^{(t)}_{i,j})}{\sum_{k\in N^\Phi_i}\text{exp}(\alpha^{(t)}_{i,k})}
\end{equation}

and 
\begin{equation}
\label{eq:upd_node}
h^{(t+1)}_i = \text{ReLU}( W_{v} h^{(t)}_i + W_{n} \sum_{j\in N^\Phi_i} a^{(t)}_{i,j} \cdot ( h^{(t)}_j \oplus r^{(t)}_{i,j} ) )
\end{equation} 

\subsection{Aggregating edge embedding}
\dan{TODO}
\begin{equation}
\label{eq:upd_edge}
%X = XZ,\;\;\; \text{s.t. } diag(Z) = 0.
X = XZ.
\end{equation} 


\subsection{Dealing with over-smoothing via adaptive depth}
\dan{TODO:relate this example to the example figure used in preliminary section}
Plain GCNs often degrades rapidly as number of layers increases \cite{LiHW18,abs-1904-03751}. Stacking four or more layers into GCNs cause over-smoothing problem and features of vertices tends to converge to the same value. In heterogeneous graphs, this problem is even more severe. Let's consider again meta-path $APCPA$. A two layer $a_1 a_2 a_3$ propagation on the shrank $A-A$ homo-graph actually passes across \textbf{8} edges on the underlying heterogeneous graph, which is a terrible number of layers for plain graph convolution networks.
To alleviate the over-smoothing problem, in equation \ref{eq:upd_node} and \ref{eq:upd_edge}, we have already included residual connections \cite{HeZRS16} in updating rule. And we propose to further improve the problem by creating jumping connections \cite{XuLTSKJ18}.

\begin{equation}
\label{eq:jump}
%X = XZ,\;\;\; \text{s.t. } diag(Z) = 0.
X = XZ.
\end{equation} 

\subsection{Aggregation across different meta-path semantics}
\dan{TODO}
\begin{equation}
\label{eq:mp}
%X = XZ,\;\;\; \text{s.t. } diag(Z) = 0.
X = XZ.
\end{equation}
\dan{need some intuition/justification shit}
Given a node $i$, and its set of embeddings under different semantics, a metapath aggregator is a function $\gamma$ in the form of $y_i = \gamma_\Theta(\{x_{i,1},x_{i,2},..x_{i,P}\})$, where $x_{i,p}$ is the embedding of node $i$ under metapath $p$, and $y_i$ is the aggregated embedding of node $i$ in the HIN. $\Theta$ is the learnable paremeters of the aggregator.
Here we propose several metapath aggregators. 

$\bullet$ \textbf{Attention}

$\bullet$ \textbf{Gated linear unit}
Inspired by forget gate mechanisms in recurrent networks (RNN) \dan{TODO:cite}, we compute a soft gate between 0 (low importance) and 1 (high importance) to represent different importance to each embedding entry.
We make the element-wise gated sum layer as follows:
\begin{multicols}{2}
\begin{equation*}
o_{\Phi_p}=\sigma(W_o^T Z_{\Phi_p}),
\end{equation*}\break
\begin{equation*}
\tilde{Z}_{\Phi_p}=\text{tanh}(W_z^T Z_{\Phi_p})
\end{equation*}
\end{multicols}
and 
\begin{equation}
\label{eq:mp_attn}
Z'_{\Phi_p}= o_{\Phi_p}\odot \tilde{Z}_{\Phi_p}
\end{equation}
The gated unit $o_{\Phi_p}$ with sigmoid output (ranged (0,1)) is used to control which elements are to be aggregated in the output.  
$\bullet$ \textbf{Other aggregators}
In preliminary experiments, CONCAT, MAX-POOLING and MEAN aggregators did not generate a better result, and we omit results of these aggregators in experiment section.

\subsection{HINGCN: Semi-supervised classification on Heterogeneous Information Network via Graph Convolution Networks}
After meta-path aggregation layer, the embeddings are fed to a 2-layer MLP for final output. In semi-supervised classification tasks, we minimize the Cross-Entropy loss and the model generates final prediction based on the largest output value.
Finally, HINGCN is summarized in Algorithm~\ref{alg}.

To this end, we summarize ROSC as follows.
ROSC first computes the TKNN graph $\mathcal{G}$
and the associated weight matrix $\mathcal{W}_K$. Then it applies power iteration to generate $p$
pseudo-eigenvectors that form a $p \times n$ matrix $X$.
After whitening and normalization on $X$,
the coefficient matrix $Z^*$ can be calculated by solving Eq.~\ref{eq:solution},
which further leads to the construction of a rectified similarity matrix $\tilde{Z}$.
Since $\tilde{Z}$ has the grouping effect, 
the standard spectral clustering methods (e.g., NCuts) will be finally applied to derive more robust clustering results.
The time complexity of ROSC will be no more than the standard spectral clustering methods,
which is $O(n^3)$ in general.
In the future, we will attempt to improve it. 


\begin{algorithm}
\begin{small}
\caption{HINGCN}
\label{alg}
\begin{algorithmic}[1]
\Require $S$, $k$.
\Ensure $\mathcal{C} = \{C_1, ..., C_k\}$
\State Compose edge features $r^{(0)}$ according to Equation \ref{eq:edge}
\State Compute the TKNN graph and the reachability matrix $\mathcal{W}$
\State Calculate $W = D^{-1}S$, where $D_{ii} = \sum_jS_{ij}$
%\For $j \leftarrow 1$ do
%\State$t = 0$
%\Repeat
%\State $v_j^{t+1} \leftarrow \frac{Wv_j^t}{||Wv_j^t||_1}$
%\State $ \delta^{t+1} \leftarrow |v_j^{t+1} - v_j^t|$
%\State $t$++
%\Until $||\delta_j^t+1 - \delta_j^t||_{max} \leq \epsilon$ or $t\geq T$
%\EndFor
\State Apply PI on $W$ and generate $p$ pseudo-eigenvectors $\{\bm{v}_r\}_{r=1}^p$
\State $X = \{\bm{v}_1^T; \bm{v}_2^T; ...; \bm{v}_p^T\}$; $X$ = whiten($X$)
\State Normalize each column vector $\bm{x}$ of $X$ such that $\bm{x}^T\bm{x} = 1$
\State Calculate the coefficient matrix $Z^*$ by Eq.~\ref{eq:solution}
\State Construct $\tilde{Z} = (|Z^*| + |(Z^*)^T|)/2$
\State Run standard spectral clustering methods, e.g., NCuts, with $\tilde{Z}$ as the
similarity matrix to obtain clusters $\mathcal{C} = \{C_r\}_{r=1}^k$
%\State Decode $\{C_r\}_{r=1}^k$ from $\{{\bm z_r}\}_{r=1}^k$
\State \Return $\mathcal{C} = \{C_1, ..., C_k\}$
\end{algorithmic}
\end{small}
\end{algorithm}









