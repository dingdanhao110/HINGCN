\section{Algorithm}
\label{sec:algorithm}

In this section we describe our HINGCN algorithm. Figure~\ref{figure:flow_graph}(c) shows a flow diagram of HINGCN. 
\dan{TODO:signposting}
\comment{
\subsection{Model overview}
The overall framework of ROSC is summarized as follows.

First, generate a set of pseudo-eigenvectors.
Since the standard spectral clustering methods using only the dominant eigenvectors may fail 
in the data containing multi-scale clusters,
we generate a set of pseudo-eigenvectors to fuse more useful cluster-separation information.

Second, rectify the given similarity matrix.
The failure of spectral clustering on multi-scale data originates from the inaccurate similarity matrix.
Therefore, we rectify the matrix and aim to achieve a new one
that can more accurately reflect the true similarities between objects.
%This is a major problem to be addressed.

Third, perform spectral clustering.
After rectification, the new similarity matrix will be more accurate and it will be safer to perform spectral clustering. 

These three steps will lead to robust clustering results and next we introduce each step in detail.
}

\subsection*{Pretrained Node Representations as Edge Features}
\label{sec:edge}
\dan{TODO}

\subsection*{Aggregating node embedding}

\dan{TODO}
\begin{equation}
\label{eq:update_node}
%X = XZ,\;\;\; \text{s.t. } diag(Z) = 0.
X = XZ.
\end{equation} 

\subsection*{Aggregating edge embedding}
\dan{TODO}
\begin{equation}
\label{eq:update_edge}
%X = XZ,\;\;\; \text{s.t. } diag(Z) = 0.
X = XZ.
\end{equation} 


\subsection*{Jumping connections for adaptive depth}
\dan{TODO}
\begin{equation}
\label{eq:update_edge}
%X = XZ,\;\;\; \text{s.t. } diag(Z) = 0.
X = XZ.
\end{equation} 

\subsection*{Aggregation across different meta-path semantics}
\dan{TODO}
\begin{equation}
\label{eq:update_edge}
%X = XZ,\;\;\; \text{s.t. } diag(Z) = 0.
X = XZ.
\end{equation}
\dan{need some intuition/justification shit}
Given a node $i$, and its set of embeddings under different semantics, a metapath aggregator is a function $\gamma$ in the form of $y_i = \gamma_\Theta(\{x_{i,1},x_{i,2},..x_{i,P}\})$, where $x_{i,p}$ is the embedding of node $i$ under metapath $p$, and $y_i$ is the aggregated embedding of node $i$ in the HIN. $\Theta$ is the learnable paremeters of the aggregator.
Here we propose several metapath aggregators. 

$\bullet$ \textbf{Attention}

$\bullet$ \textbf{Element-wise Gated Sum}
Inspired by forget gate mechanisms in recurrent networks (RNN) \dan{TODO:cite}, we compute a soft gate between 0 (low importance) and 1 (high importance) to represent different importance to each embedding entry.
We make the element-wise gated sum layer as follows:
\begin{multicols}{2}
\begin{equation*}
o_{\Phi_p}=\sigma(W_o^T Z_{\Phi_p}),
\end{equation*}\break
\begin{equation*}
\tilde{Z}_{\Phi_p}=\text{tanh}(W_z^T Z_{\Phi_p})
\end{equation*}
\end{multicols}
and 
\begin{equation}
\label{eq:mp_attn}
Z'_{\Phi_p}= o_{\Phi_p}\odot \tilde{Z}_{\Phi_p}
\end{equation}
The gated unit $o_{\Phi_p}$ with sigmoid output (ranged (0,1)) is used to control which elements are to be aggregated in the output.  
$\bullet$ \textbf{Other aggregators}
In preliminary experiments, CONCAT, MAX-POOLING and MEAN aggregators did not generate a better result, and we omit results of these aggregators in experiment section.

\subsection*{HINGCN: Semi-supervised classification on Heterogeneous Information Network via Graph Convolution Networks}
After meta-path aggregation layer, the embeddings are fed to a 2-layer MLP for final output. In semi-supervised classification tasks, we minimize the Cross-Entropy loss and the model generates final prediction based on the largest output value.
Finally, HINGCN is summarized in Algorithm~\ref{alg}.

To this end, we summarize ROSC as follows.
ROSC first computes the TKNN graph $\mathcal{G}$
and the associated weight matrix $\mathcal{W}_K$. Then it applies power iteration to generate $p$
pseudo-eigenvectors that form a $p \times n$ matrix $X$.
After whitening and normalization on $X$,
the coefficient matrix $Z^*$ can be calculated by solving Eq.~\ref{eq:solution},
which further leads to the construction of a rectified similarity matrix $\tilde{Z}$.
Since $\tilde{Z}$ has the grouping effect, 
the standard spectral clustering methods (e.g., NCuts) will be finally applied to derive more robust clustering results.
The time complexity of ROSC will be no more than the standard spectral clustering methods,
which is $O(n^3)$ in general.
In the future, we will attempt to improve it. 


\begin{algorithm}
\begin{small}
\caption{HINGCN}
\label{alg}
\begin{algorithmic}[1]
\Require $S$, $k$.
\Ensure $\mathcal{C} = \{C_1, ..., C_k\}$
\State Compute the TKNN graph and the reachability matrix $\mathcal{W}$
\State Calculate $W = D^{-1}S$, where $D_{ii} = \sum_jS_{ij}$
%\For $j \leftarrow 1$ do
%\State$t = 0$
%\Repeat
%\State $v_j^{t+1} \leftarrow \frac{Wv_j^t}{||Wv_j^t||_1}$
%\State $ \delta^{t+1} \leftarrow |v_j^{t+1} - v_j^t|$
%\State $t$++
%\Until $||\delta_j^t+1 - \delta_j^t||_{max} \leq \epsilon$ or $t\geq T$
%\EndFor
\State Apply PI on $W$ and generate $p$ pseudo-eigenvectors $\{\bm{v}_r\}_{r=1}^p$
\State $X = \{\bm{v}_1^T; \bm{v}_2^T; ...; \bm{v}_p^T\}$; $X$ = whiten($X$)
\State Normalize each column vector $\bm{x}$ of $X$ such that $\bm{x}^T\bm{x} = 1$
\State Calculate the coefficient matrix $Z^*$ by Eq.~\ref{eq:solution}
\State Construct $\tilde{Z} = (|Z^*| + |(Z^*)^T|)/2$
\State Run standard spectral clustering methods, e.g., NCuts, with $\tilde{Z}$ as the
similarity matrix to obtain clusters $\mathcal{C} = \{C_r\}_{r=1}^k$
%\State Decode $\{C_r\}_{r=1}^k$ from $\{{\bm z_r}\}_{r=1}^k$
\State \Return $\mathcal{C} = \{C_1, ..., C_k\}$
\end{algorithmic}
\end{small}
\end{algorithm}









