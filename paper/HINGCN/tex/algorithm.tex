\section{Algorithm}
\label{sec:algorithm}

In this section we describe our HINGCN algorithm. Figure~\ref{figure:flow_graph}(c) shows a flow diagram of HINGCN. 
\dan{TODO:signposting}

\subsection{Edge features as supplementary information}
\label{sec:edge}
We claim that information is loss if we only consider adjacency of target type vertices. For example, in long tailed meta-path $APCPA$, a shrunk $A$-$A$ adjacency loses information from intermediate $PCP$ nodes and cannot distinguish between two instance $p_1 = (a_1 p_1 c_1 p_2 a_2)$ and $p_2 = (a_1 p_3 c_2 p_4 a_2)$. Therefore we propose to add this information back by adding additional edge feature to homogeneous $A$-$A$ edges. These edge features are composed of three different components, namely intermediate node embeddings, path count and PathSim\citep{SunHYYW11}.

\noindent{\small$\bullet$}\textbf{Intermediate node embedding}. 

\noindent{\small$\bullet$}\textbf{Path Count}. Different object pairs have different number of path instance between them and this number may reflect object similarity under a given meta-path. We propose to record this count of path between objects $x_u$ and $x_v$  under meta-path $\Phi$ by: 
\begin{equation*}
c(x_u,x_v) = \vert\{ p_{x_u \rightsquigarrow x_v}:p_{x_u \rightsquigarrow x_v} \vdash \Phi \}\vert
\end{equation*}

\noindent{\small$\bullet$}\textbf{PathSim}. 
PathSim is proposed in \citep{SunHYYW11} to measure similarity between two objects of a same type on a heterogeneous graph. Given a symmetric meta-path $\Phi$, PathSim between $x_u$ and $x_v$ is:
\begin{equation*}
s(x_u,x_v) = \frac{2\times\vert\{ p_{x_u \rightsquigarrow x_v}:p_{x_u \rightsquigarrow x_v} \vdash \Phi \}\vert}{\vert\{ p_{x_u \rightsquigarrow x_u}:p_{x_u \rightsquigarrow x_u} \vdash \Phi \}\vert +\vert\{ p_{x_v \rightsquigarrow x_v}:p_{x_v \rightsquigarrow x_v} \vdash \Phi \}\vert }
\end{equation*}
\dan{TODO}

\subsection{Aggregating node embedding}

\dan{TODO}
\begin{equation}
\label{eq:update_node}
%X = XZ,\;\;\; \text{s.t. } diag(Z) = 0.
X = XZ.
\end{equation} 

\subsection{Aggregating edge embedding}
\dan{TODO}
\begin{equation}
\label{eq:update_edge}
%X = XZ,\;\;\; \text{s.t. } diag(Z) = 0.
X = XZ.
\end{equation} 


\subsection{Dealing with over-smoothing via adaptive depth}
\dan{TODO}
Inspired by DenseNet and ResNet, we alleviate the problem of over-smoothing over graph convolutions by creating jumping connections.
\begin{equation}
\label{eq:update_edge}
%X = XZ,\;\;\; \text{s.t. } diag(Z) = 0.
X = XZ.
\end{equation} 

\subsection{Aggregation across different meta-path semantics}
\dan{TODO}
\begin{equation}
\label{eq:update_edge}
%X = XZ,\;\;\; \text{s.t. } diag(Z) = 0.
X = XZ.
\end{equation}
\dan{need some intuition/justification shit}
Given a node $i$, and its set of embeddings under different semantics, a metapath aggregator is a function $\gamma$ in the form of $y_i = \gamma_\Theta(\{x_{i,1},x_{i,2},..x_{i,P}\})$, where $x_{i,p}$ is the embedding of node $i$ under metapath $p$, and $y_i$ is the aggregated embedding of node $i$ in the HIN. $\Theta$ is the learnable paremeters of the aggregator.
Here we propose several metapath aggregators. 

$\bullet$ \textbf{Attention}

$\bullet$ \textbf{Gated linear unit}
Inspired by forget gate mechanisms in recurrent networks (RNN) \dan{TODO:cite}, we compute a soft gate between 0 (low importance) and 1 (high importance) to represent different importance to each embedding entry.
We make the element-wise gated sum layer as follows:
\begin{multicols}{2}
\begin{equation*}
o_{\Phi_p}=\sigma(W_o^T Z_{\Phi_p}),
\end{equation*}\break
\begin{equation*}
\tilde{Z}_{\Phi_p}=\text{tanh}(W_z^T Z_{\Phi_p})
\end{equation*}
\end{multicols}
and 
\begin{equation}
\label{eq:mp_attn}
Z'_{\Phi_p}= o_{\Phi_p}\odot \tilde{Z}_{\Phi_p}
\end{equation}
The gated unit $o_{\Phi_p}$ with sigmoid output (ranged (0,1)) is used to control which elements are to be aggregated in the output.  
$\bullet$ \textbf{Other aggregators}
In preliminary experiments, CONCAT, MAX-POOLING and MEAN aggregators did not generate a better result, and we omit results of these aggregators in experiment section.

\subsection{HINGCN: Semi-supervised classification on Heterogeneous Information Network via Graph Convolution Networks}
After meta-path aggregation layer, the embeddings are fed to a 2-layer MLP for final output. In semi-supervised classification tasks, we minimize the Cross-Entropy loss and the model generates final prediction based on the largest output value.
Finally, HINGCN is summarized in Algorithm~\ref{alg}.

To this end, we summarize ROSC as follows.
ROSC first computes the TKNN graph $\mathcal{G}$
and the associated weight matrix $\mathcal{W}_K$. Then it applies power iteration to generate $p$
pseudo-eigenvectors that form a $p \times n$ matrix $X$.
After whitening and normalization on $X$,
the coefficient matrix $Z^*$ can be calculated by solving Eq.~\ref{eq:solution},
which further leads to the construction of a rectified similarity matrix $\tilde{Z}$.
Since $\tilde{Z}$ has the grouping effect, 
the standard spectral clustering methods (e.g., NCuts) will be finally applied to derive more robust clustering results.
The time complexity of ROSC will be no more than the standard spectral clustering methods,
which is $O(n^3)$ in general.
In the future, we will attempt to improve it. 


\begin{algorithm}
\begin{small}
\caption{HINGCN}
\label{alg}
\begin{algorithmic}[1]
\Require $S$, $k$.
\Ensure $\mathcal{C} = \{C_1, ..., C_k\}$
\State Compute the TKNN graph and the reachability matrix $\mathcal{W}$
\State Calculate $W = D^{-1}S$, where $D_{ii} = \sum_jS_{ij}$
%\For $j \leftarrow 1$ do
%\State$t = 0$
%\Repeat
%\State $v_j^{t+1} \leftarrow \frac{Wv_j^t}{||Wv_j^t||_1}$
%\State $ \delta^{t+1} \leftarrow |v_j^{t+1} - v_j^t|$
%\State $t$++
%\Until $||\delta_j^t+1 - \delta_j^t||_{max} \leq \epsilon$ or $t\geq T$
%\EndFor
\State Apply PI on $W$ and generate $p$ pseudo-eigenvectors $\{\bm{v}_r\}_{r=1}^p$
\State $X = \{\bm{v}_1^T; \bm{v}_2^T; ...; \bm{v}_p^T\}$; $X$ = whiten($X$)
\State Normalize each column vector $\bm{x}$ of $X$ such that $\bm{x}^T\bm{x} = 1$
\State Calculate the coefficient matrix $Z^*$ by Eq.~\ref{eq:solution}
\State Construct $\tilde{Z} = (|Z^*| + |(Z^*)^T|)/2$
\State Run standard spectral clustering methods, e.g., NCuts, with $\tilde{Z}$ as the
similarity matrix to obtain clusters $\mathcal{C} = \{C_r\}_{r=1}^k$
%\State Decode $\{C_r\}_{r=1}^k$ from $\{{\bm z_r}\}_{r=1}^k$
\State \Return $\mathcal{C} = \{C_1, ..., C_k\}$
\end{algorithmic}
\end{small}
\end{algorithm}









