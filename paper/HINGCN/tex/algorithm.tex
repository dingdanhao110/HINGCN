\section{Algorithm}
\label{sec:algorithm}

In this section we describe our HINGCN algorithm. Figure~\ref{figure:flow_graph}(c) shows a flow diagram of HINGCN. 
\dan{TODO:signposting}

\subsection{Edge features as supplementary information}
\label{sec:edge}
According to research findings of Xu et al. \cite{XuHLJ19}, discriminative power of a graph neural network determines its representation capability.
We claim that information is loss if we only consider adjacency of target type vertices.
Let's consider again the example shown in Fig.~\ref{figure:example}. In long tailed meta-path $APCPA$, a shrinking $A$-$A$ adjacency loses information from intermediate $PCP$ nodes. And a graph neural network using only adjacency information cannot distinguish between two instances $p_1 = (a_1 p_1 c_1 p_2 a_4)$ and $p_2 = (a_1 p_2 c_2 p_3 a_4)$. Instead, both instances are represented as edge $\langle a_1 a_4\rangle$.
Therefore we propose to improve expressivity of our network by adding additional edge feature to homogeneous $A$-$A$ edges. These edge features are designed to effectly distinguish different intermediate nodes between meta-path instances.
We propose edge features that is composed of three different components, namely path count, intermediate node embeddings and PathSim\citep{SunHYYW11}.

\noindent{\small$\bullet$}\textbf{Path Count}. Different object pairs have different numbers of path instance between them and these numbers may reflect object similarity under a given meta-path. We propose to record count of path between objects $x_u$ and $x_v$ under meta-path $\Phi$ by: 
\begin{equation*}
c(x_u,x_v) = \vert\{ p_{x_u \rightsquigarrow x_v}:p_{x_u \rightsquigarrow x_v} \vdash \Phi \}\vert
\end{equation*}

\noindent{\small$\bullet$}\textbf{Intermediate object embedding}. 
To discriminate between different intermediate objects, we propose to use a sum of pre-trained node embedding as a component of edge feature. This node embedding can be trained from heterogeneous embedding algorithms such as \cite{GroverL16,DongCS17}. We favor sum over other aggregators since this operation possibly preserves maximum discriminant power on graphs \citep{XuHLJ19}.
Given a meta-path instance $p = (x_1x_2\ldots x_{l+1})$, we calculate
\begin{equation*}
o(x_1,x_{l+1}) = \dfrac{1}{(l+1) \cdot c(x_1,x_{l+1})} \sum\limits_{i=1}^{l+1} e_i
\end{equation*}
where $e_i$ is embedding of object $x_i$. Division of number of vertices $l+1$ and path count $c(x_u,x_v)$ is proposed to normalize intermediate object embedding to the scale of input object embeddings. Since path count is extracted and recorded as $c(x_u,x_v)$, this does no result in extra loss of information.

\noindent{\small$\bullet$}\textbf{PathSim}. 
PathSim is proposed in \citep{SunHYYW11} to measure similarity between two objects of a same type on a heterogeneous graph. Given a symmetric meta-path $\Phi$, PathSim between $x_u$ and $x_v$ is:
\begin{equation*}
s(x_u,x_v) = \frac{2\times\vert\{ p_{x_u \rightsquigarrow x_v}:p_{x_u \rightsquigarrow x_v} \vdash \Phi \}\vert}{\vert\{ p_{x_u \rightsquigarrow x_u}:p_{x_u \rightsquigarrow x_u} \vdash \Phi \}\vert +\vert\{ p_{x_v \rightsquigarrow x_v}:p_{x_v \rightsquigarrow x_v} \vdash \Phi \}\vert }
\end{equation*}

The edge feature between objects $x_u$ and $x_v$ under meta-path $\Phi$ is thereby concatenated as:
\begin{equation}
\label{eq:edge}
r^{(0)}_\Phi(x_u,x_v) = c(x_u,x_v)\oplus o(x_u,x_v)\oplus s(x_u,x_v)
\end{equation}

\subsection{Aggregating node embedding}
We propose a modified version of scaled dot product attention mechanism to aggregate node embedding from its neighbor. Given a meta-path $\Phi$, we update vertex embedding as follow. First we apply 2 layer MLPs on node embedding and its neighbor respectively for scaling purpose. The query vector of node $i$ is
\begin{equation}
\label{eq:query}
q_i= W_{a1}^{(t)}(\text{tanh}(W_{a2}^{(t)}h^{(t)}_i ))
\end{equation}
And key vector $k_{i,j}$ is calculated as:
\begin{equation}
\label{eq:key}
k_{i,j} = W_{a3}^{(t)}(\text{tanh}(W_{a4}^{(t)}(h^{(t)}_j \oplus r^{(t)}_{i,j}) ))
\end{equation}
And the value vector of neighbor $j$ is calculated as: 
\begin{equation}
\label{eq:value}
v^{(t)}_{i,j} = W^{(t)}( h^{(t)}_j \oplus r^{(t)}_{i,j} )
\end{equation} 
where edge feature $r^{(t)}_{i,j}$ is concatenated to vertex feature of the neighbors, in order to provide additional information. 

It is worth noticing that due to unique feature of each edge, both key and value vectors are no longer universal for different query nodes. We believe this uniqueness improves expressivity of self-attention mechanism for HIN embedding tasks.

\noindent Afterwards, the importance of a neighbor $j$ to object $i$ is calculated as:
\begin{equation}
\label{eq:dot}
\tilde{a}^{(t)}_{i,j} = q_i^T \cdot k_{i,j}
\end{equation}
Then we normalize this importance coefficient by softmax function and calculate attention coefficient as:
\begin{equation}
\label{eq:softmax}
a^{(t)}_{i,j} = \text{softmax}(\tilde{a}^{(t)}_{i,j}) = \dfrac{\text{exp}(\tilde{a}^{(t)}_{i,j})}{\sum_{k\in N^\Phi_i}\text{exp}(\tilde{a}^{(t)}_{i,k})}
\end{equation}
Finally the output of the node aggregation layer is a weighted sum of value vectors. 
\begin{equation}
\label{eq:upd_node}
h^{(t+1)}_i = \text{ReLU}( W_{h1}\cdot \sum_{j\in N^\Phi_i} a^{(t)}_{i,j} \cdot v^{(t)}_{i,j} + W_{h2}\cdot h^{(t)}_i) 
\end{equation} 
Although a vertex $i$ is always in its own meta-path neighbor, we still explicitly add term $h^{(t)}_i$ in equation \ref{eq:upd_node} following GraphSAGE\cite{HamiltonYL17}.

\subsection{Aggregating edge embedding}
It is a natural idea to treat edge embeddings as parameters and update on pre-trained edge embeddings by gradient descend. However, this is not feasible in our experiment condition. Unlike language models where only a small portion of pre-trained embeddings are involved, in transductive settings, all edge embeddings we processed contribute to training process. 
Due to the fact that the shrinking homograph is much denser than original graph, treating these edge embeddings as parameters would drastically increase number of parameters involved the model. A dense homogeneous adjacency could result in an update of billions of parameters in single epoch, especially in long tail meta-paths like $APCPA$ in DBLP dataset, where density of adjacency matrix could reach \dan{30\%?} or more.

Therefore, we propose another way to fine-tune edge embeddings by allowing convolution on edge embeddings. Pre-trained embeddings are served as initial feature, and the edge convolution layer is allowed to update edge embeddings by aggregating information from its endpoint nodes. 
\begin{equation}
\label{eq:upd_edge}
r^{(t+1)}_{i,j} = \text{ReLU}(W_{r1}\cdot (h^{(t)}_i \oplus h^{(t)}_j \oplus r^{(t)}_{i,j}) + W_{r2}\cdot r^{(t)}_{i,j}) 
\end{equation} 

Term $W_{r2}\cdot r^{(t)}_{i,j}$ at the end of the equation serve as a shortcut residual, in order to guarantee a feasible convergence.

In multi-layer HINGCN, we propose an alternate update scheme of node embedding and edge embedding. 
\subsection{Dealing with over-smoothing via adaptive depth}
Plain GCNs often degrades rapidly as number of layers increases \cite{LiHW18,abs-1904-03751}. Stacking four or more layers into GCNs cause over-smoothing problem and features of vertices tends to converge to the same value. In heterogeneous graphs, this problem is even more severe. Let's consider again meta-path $APCPA$ in Fig.~\ref{figure:example}. A two layer $a_1 a_2 a_3$ propagation on the shrinking $A-A$ homo-graph actually passes across \textbf{8} edges on the underlying heterogeneous graph, which is a terrible number of layers for plain graph convolution networks.
In previous sections, our model already address this problem by including edge features in node update and updating edge feature. To further alleviate the over-smoothing problem, in equation \ref{eq:upd_node} and \ref{eq:upd_edge}, we have already included residual connections \cite{HeZRS16} in updating rule. And we propose to further improve the problem by creating jumping connections \cite{XuLTSKJ18}.
\begin{equation}
\label{eq:jump}
h^{(final)}_i = h^{(0)}_i \oplus h^{(1)}_i \cdots \oplus h^{(t)}_i
\end{equation} 

\subsection{Aggregation across different meta-path semantics}
\label{sec:mp_aggr}
\dan{need some intuition/justification shit}
Given a node $i$, and its set of embeddings under different semantics, a metapath aggregator is a function $\gamma$ in the form of $y_i = \gamma_\Theta(\{x_{i,1},x_{i,2},..x_{i,P}\})$, where $x_{i,p}$ is the embedding of node $i$ under metapath $p$, and $y_i$ is the aggregated embedding of node $i$ in the HIN. $\Theta$ is the learnable paremeters of the aggregator.
Here we propose several metapath aggregators. 

$\bullet$ \textbf{Attention}
\dan{cite} The first option we consider is scaled dot-product attention introduced in Transformer. 
\begin{equation}
\label{eq:mp_mlp}
\tilde{h}^{\Phi_p}_i = W_{m1} \cdot \text{tanh}(W_{m2} \cdot h^{\Phi_p}_i)
\end{equation}

\begin{equation}
\label{eq:mp_attn}
\tilde{w}_i^{\Phi_p} = W_w \cdot \sigma(\tilde{h}^{\Phi_p}_i)
\end{equation}

\begin{equation}
\label{eq:mp_soft}
w_i^{\Phi_p} = \text{softmax}(\tilde{w}_i^{\Phi_p}) = \dfrac{\text{exp}(\tilde{w}_i^{\Phi_p})}{\sum_q \text{exp}(\tilde{w}_i^{\Phi_q})}
\end{equation}

After the fully-connected self-attention, we propose to reduce across different meta-path semantics by \textbf{Sum} operation.
\begin{equation}
\label{eq:mp_asum}
z_i = \text{ReLU}(\sum_p w_i^{\Phi_p} \cdot h^{\Phi_p}_i)
\end{equation}

$\bullet$ \textbf{Gated linear unit}
Inspired by forget gate mechanisms in recurrent networks (RNN) \dan{TODO:cite}, we compute a soft gate between 0 (low importance) and 1 (high importance) to represent different importance to each embedding entry.
We make the element-wise gated sum layer as follows:
\begin{multicols}{2}
\begin{equation}
o^{\Phi_p}_i=\sigma(W_o \cdot h^{\Phi_p}_i)
\end{equation}\break
\begin{equation}
\tilde{h}^{\Phi_p}_i=\text{tanh}(W_z \cdot h^{\Phi_p}_i)
\end{equation}
\end{multicols}
Finally, we have
\begin{equation}
\label{eq:mp_sum}
{z}_i= \sum_p o^{\Phi_p}_i\odot \tilde{h}^{\Phi_p}_i
\end{equation}

The gated unit $o_{\Phi_p}$ with sigmoid output (ranged (0,1)) is used to control which elements are to be aggregated in the output. 
 
$\bullet$ \textbf{Other aggregators}
In preliminary experiments, \textbf{Concat}, \textbf{Max-Pooling} and \textbf{Mean} aggregators did not generate a better result, and we omit results of these aggregators in experiment section.

\subsection{HINGCN: Semi-supervised classification on Heterogeneous Information Network via Graph Convolution Networks}
After meta-path aggregation layer, the embeddings are fed to a 2-layer MLP for final output. In semi-supervised classification tasks, we minimize the Cross-Entropy loss and the model generates final prediction based on the largest output value.
\begin{equation}
\label{eq:loss}
\mathcal{L}(W)=   - \sum_{l\in \mathcal{Y}_L}Y^l ln(z^l)   + \lambda || W ||_2
\end{equation}
$ \mathcal{Y}_L$ is the set of labeled nodes in training set. $W$ represents the weight matrices used in the layers of our HINGCN. We use $L_2$ normalization for parameters involved.

To this end, we give a summary of overall process of HINGCN.
The part of embedding generation is summarized in Algorithm~\ref{alg}.
HINGCN first computes initial edge feature $r^{(0)}_{\Phi_p}$ for each edge under each meta-path. Then it applies an alternative update of node embedding and edge embedding.
After that each node fuses a meta-path specific embedding by jumping connections. These embeddings are further aggregated using either meta-path level attention or gated linear units. Finally, embeddings are fed to a two layer MLP and a softmax layer for classification output.

\begin{algorithm}
%\begin{small}
\caption{HINGCN Embedding Generation}
\label{alg}
\begin{algorithmic}[1]
\Require HIN $G=(V,E)$;
labeled type $T$;
meta-path set ${\Phi_0,\Phi_1,\ldots ,\Phi_P}$;
an initial node representation $\{ f_i,\forall x_i \in X_T \}$;
number of aggregation layers $L$;
\Ensure Final node embeddings $Z = \{z_1, ..., z_i\}$
\State $h^{(0)}_i \leftarrow f_i$, $\forall x_i \in X_T $
\For {$p \leftarrow 0$ to $P$}
\State Compose edge features $r^{(0)}_{\Phi_p}$ according to Eq.\ref{eq:edge}.
\EndFor
\For {$p \leftarrow 0$ to $P$}
 \For {$t \leftarrow 0$ to $L-1$}
 %Update node embeddings:
  \For {$x_i \in X_T$}
  \State $q_i= W_{a1}^{(t)}(\text{tanh}(W_{a2}^{(t)}h^{(t)}_i ))$
 \For {$x_j \in N^{\Phi_p}_i$}
  \State $k_{i,j} = W_{a3}^{(t)}(\text{tanh}(W_{a4}^{(t)}(h^{(t)}_j \oplus r^{(t)}_{i,j}) ))$
  \State $v^{(t)}_{i,j} = W^{(t)}( h^{(t)}_j \oplus r^{(t)}_{i,j} )$
  %Calculate importance of j to i
  \State $\tilde{a}^{(t)}_{i,j} = q_i^T \cdot k_{i,j}$
  %Calculate the attention coefficient 
  \State $a^{(t)}_{i,j} = \text{softmax}(\tilde{a}^{(t)}_{i,j})$
 \EndFor
  \State $h^{(t+1)}_i = \text{ReLU}(W_{h1}\cdot \sum_{j\in N^{\Phi_p}_i} a^{(t)}_{i,j} \cdot v^{(t)}_{i,j}+ W_{h2}\cdot h^{(t)}_i) $
  \EndFor

  %\State Update edge embeddings:
  \For {$e \in E$}
  \State $(x_i,x_j) \leftarrow \text{Endpoints}(e)$
  \State $r^{(t+1)}_{i,j} = \text{ReLU}(W_{r1}\cdot (h^{(t)}_i \oplus h^{(t)}_j \oplus r^{(t)}_{i,j}) + W_{r2}\cdot r^{(t)}_{i,j})$
  \EndFor
 \EndFor
 \State $h^{\Phi_p}_i = h^{(0)}_i \oplus h^{(1)}_i \cdots \oplus h^{(t)}_i$
\EndFor
\State Aggregate meta-path specific embeddings and compute ${z}_i$ according to \textbf{Attention} (Eq.\ref{eq:mp_asum}) or \textbf{Gated Linear Units} (Eq.\ref{eq:mp_sum})
\State \Return $Z = \{z_1, ..., z_i\}$ 
\end{algorithmic}
%\end{small}
\end{algorithm}









